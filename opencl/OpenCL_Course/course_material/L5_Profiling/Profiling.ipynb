{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da6d105-2df5-40a6-9baa-02497fcef0d8",
   "metadata": {},
   "source": [
    "# Measuring peformance in OpenCL applications\n",
    "\n",
    "Having an understanding of how well OpenCL applications perform is a vital part of the development process. The two main tools, **profiling** and **tracing** collect information about how well an application is performing. **Profiling** is the statistical collection of the cumulative time that threads spend in each program component. **Tracing** is the collection of both **when** and **for how long** threads spend in each application component. While it is true that many vendors have largely abandoned their OpenCL performance measurement tools, the OpenCL standard itself provides a profiling interface and there are still a few open-source and commercial tools available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d07024-044f-4c11-a9e9-2c94222ca25d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Event based timing\n",
    "\n",
    "Events in OpenCL are used with streams to check the progress of work that has been submitted and establish dependencies between workflows. They can also be used to time the execution of work, such as kernels and memory copies. Here is how Events fit into the picture of an OpenCL application.\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right:auto; width:70%;\">\n",
    "    <img style=\"vertical-\n",
    "                align:middle\" src=\"../images/opencl_components.svg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Components of an OpenCL application. Events are associated with command queues, and provide a way to time the duration of work in a command queue. </figcaption>\n",
    "</figure>\n",
    "\n",
    "Command queues have the ability to capture timing information for work submitted. In order to time commands submitted to an OpenCL command queue we enable a profiling flag called **CL_QUEUE_PROFILING_ENABLE** at command queue creation. Then time elapsed may be extracted directly from an event. In the code [mat_mult_profiling.cpp](mat_mult_profiling.cpp) we set the profiling flag to CL_TRUE.\n",
    "\n",
    "```C++\n",
    "    // mat_mult_profiling.cpp source\n",
    "\n",
    "    // Do we enable profiling?\n",
    "    cl_bool profiling = CL_TRUE;\n",
    "```\n",
    "\n",
    "Then from within **h_create_command_queues** in <a href=\"../include/cl_helper.hpp\">cl_helper.hpp</a>, the profiling flag **CL_QUEUE_PROFILING_ENABLE** is incorporated into the command queue properties and passed to either [clCreateCommandQueue](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clCreateCommandQueue.html) from the OpenCL 1.2 API or [clCreateCommandQueueWithProperties](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clCreateCommandQueueWithProperties.html) from the OpenCL 2.0 API.\n",
    "\n",
    "```C++\n",
    "    // cl_helper.hpp source\n",
    "\n",
    "#ifdef CL_VERSION_2_0\n",
    "        // Check to see what version the device supports\n",
    "        cl_float device_ver = h_get_device_ver(device);\n",
    "\n",
    "        if (device_ver>=2.0) {\n",
    "            // Create the command queue the OpenCL 2.0 way\n",
    "            cl_queue_properties queue2_props[] = {\n",
    "                CL_QUEUE_PROPERTIES, queue_properties,\n",
    "                0\n",
    "            };\n",
    "            // Use the OpenCL 2.0 API    \n",
    "            command_queues[n] = clCreateCommandQueueWithProperties(\n",
    "                context,\n",
    "                device,\n",
    "                queue2_props,\n",
    "                &errcode    \n",
    "            );\n",
    "            h_errchk(errcode, \"Creating an OpenCL 2.0 command queue with properties\");  \n",
    "\n",
    "        } else \n",
    "#endif\n",
    "        { \n",
    "            // Create the command queue the OpenCL 1.2 way\n",
    "            command_queues[n] = clCreateCommandQueue(\n",
    "                context,\n",
    "                device,\n",
    "                queue_properties,\n",
    "                &errcode    \n",
    "            );\n",
    "\n",
    "            h_errchk(errcode, \"Creating an OpenCL 1.2 command queue\");  \n",
    "        }\n",
    "```\n",
    "\n",
    "The function [clGetEventProfilingInfo](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clGetEventProfilingInfo.html) extracts information such as start and end walltimes (in nanoseconds) for an OpenCL event associated with a queued command. We use the helper function **h_get_event_time_ms** in <a href=\"../include/cl_helper.hpp\">cl_helper.hpp</a> to extract the elapsed time.\n",
    "\n",
    "```C++\n",
    "\n",
    "// cl_helper.hpp source\n",
    "\n",
    "cl_double h_get_event_time_ms(\n",
    "        cl_event *event, \n",
    "        const char* message, \n",
    "        size_t* nbytes) {\n",
    "    \n",
    "    // Make sure the event has finished\n",
    "    h_errchk(clWaitForEvents(1, event), message);\n",
    "    \n",
    "    // Start and end times\n",
    "    cl_ulong t1, t2;\n",
    "        \n",
    "    // Fetch the start and end times in nanoseconds\n",
    "    h_errchk(\n",
    "        clGetEventProfilingInfo(\n",
    "            *event,\n",
    "            CL_PROFILING_COMMAND_START,\n",
    "            sizeof(cl_ulong),\n",
    "            &t1,\n",
    "            NULL\n",
    "        ),\n",
    "        \"Fetching start time for event\"\n",
    "    );\n",
    "\n",
    "    h_errchk(\n",
    "        clGetEventProfilingInfo(\n",
    "            *event,\n",
    "            CL_PROFILING_COMMAND_END,\n",
    "            sizeof(cl_ulong),\n",
    "            &t2,\n",
    "            NULL\n",
    "        ),\n",
    "        \"Fetching end time for event\"\n",
    "    );\n",
    "    \n",
    "    // Convert the time into milliseconds\n",
    "    cl_double elapsed = (cl_double)(t2-t1)*(cl_double)1.0e-6;\n",
    "        \n",
    "    // Print the timing message if necessary\n",
    "    if (strlen(message)>0) {\n",
    "        std::printf(\"Time for event \\\"%s\\\": %.3f ms\", message, elapsed);\n",
    "        \n",
    "        // Print transfer rate if nbytes is specified\n",
    "        if (nbytes != NULL) {\n",
    "            cl_double io_rate_MBs = h_get_io_rate_MBs(\n",
    "                elapsed, \n",
    "                *nbytes\n",
    "            );\n",
    "            std::printf(\" (%.2f MB/s)\", io_rate_MBs);\n",
    "        }\n",
    "        std::printf(\"\\n\");\n",
    "    }\n",
    "    \n",
    "    return elapsed;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41146a0-7600-407e-899d-43a26344927c",
   "metadata": {},
   "source": [
    "Every command submitted to a command queue may have an event associated with it. We can use this method to extract the time taken for work to complete.\n",
    "\n",
    "### Instrumenting the buffer copy\n",
    "\n",
    "We construct a **cl_event** object and use that event to collect timing information. For example, during writes to a device buffer we pass in the event to collect timing information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521d954-f071-4bdb-8b9f-1e87469eff0d",
   "metadata": {},
   "source": [
    "```C++\n",
    "    // mat_mult.cpp source\n",
    "   cl_event io_event;\n",
    "\n",
    "    H_ERRCHK(\n",
    "        clEnqueueWriteBuffer(\n",
    "            command_queue,\n",
    "            A_d,\n",
    "            blocking,\n",
    "            0,\n",
    "            nbytes_A,\n",
    "            A_h,\n",
    "            0,\n",
    "            NULL,\n",
    "            &io_event\n",
    "        ) \n",
    "    );\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe99abd7-4aae-4c72-86b6-5f216b7ac590",
   "metadata": {},
   "source": [
    "Then, we use **h_get_event_time_ms** to extract the elapsed time and print out the transfer rate.\n",
    "\n",
    "```C++\n",
    "    // Time how long it takes to complete event\n",
    "    cl_double upload_A_ms = h_get_event_time_ms(\n",
    "        &io_event, \n",
    "        \"Uploading Buffer A\",\n",
    "        &nbytes_A\n",
    "    );\n",
    "```\n",
    "\n",
    "The buffer copies from **B_h** to **B_d**, and then from **C_d** to **C_h** are also instrumented in a similar way. From the previous call to **h_get_event_time_ms** we know **io_event** is in a complete state, so we can reuse it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06f835-46d4-4860-b3a9-1479d730be2d",
   "metadata": {},
   "source": [
    "### Instrumenting the kernel\n",
    "\n",
    "The **[clEnqueueNDRangeKernel](https://www.khronos.org/registry/OpenCL/sdk/3.0/docs/man/html/clEnqueueNDRangeKernel.html)** command accepts a pointer to an OpenCL Event. This event then tracks the time taken for the kernel to complete. \n",
    "\n",
    "```C++\n",
    "    // Event for the kernel\n",
    "    cl_event kernel_event;\n",
    "    \n",
    "    // Now enqueue the kernel\n",
    "    H_ERRCHK(\n",
    "        clEnqueueNDRangeKernel(\n",
    "            command_queue,\n",
    "            kernel,\n",
    "            work_dim,\n",
    "            NULL,\n",
    "            global_size,\n",
    "            local_size,\n",
    "            0,\n",
    "            NULL,\n",
    "            &kernel_event\n",
    "        ) \n",
    "    );\n",
    "\n",
    "    // Time how long it takes to complete event\n",
    "    cl_double run_kernel_ms = h_get_event_time_ms(\n",
    "        &kernel_event, \n",
    "        \"Running kernel\",\n",
    "        NULL\n",
    "    );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bd3ff9-409d-4f1a-a692-24e512e58bcb",
   "metadata": {},
   "source": [
    "In this manner we instrument the uploads, downloads, and kernel execution in the source file [mat_mult_profiling.cpp](mat_mult_profiling.cpp). Now we run the instrumented code and print out the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64468ad-d6f3-4b8a-9fa9-5d7fd74a28c0",
   "metadata": {},
   "source": [
    "## Compile and run the appliciation\n",
    "\n",
    "The makefile is set to compile the instrumented example [mat_mult_profiling.cpp](mat_mult_profiling.cpp). The program creates and fills matrices **A_d** and **B_d** with random numbers in the range [0-1] and then uses OpenCL to compute the solution in matrix **C_d**. The matrices are written to the following files in binary format:\n",
    "\n",
    "* arrayA.dat\n",
    "* arrayB.dat\n",
    "* arrayC.dat\n",
    "\n",
    "On your terminal change directory to **L5_Profiling** and compile and run with these commands (without the exclamation mark !)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63b5c7f6-00d0-4630-bae7-79209769b235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm -rf *.exe\n",
      "CC -g -fopenmp -O2 -I../include mat_mult_profiling.cpp -o mat_mult_profiling.exe -lOpenCL\n",
      "\t               name: gfx1035 \n",
      "\t     Device version: OpenCL 2.0  \n",
      "\t global memory size: 536 MB\n",
      "\t    max buffer size: 456 MB\n",
      "\t     max local size: (1024,1024,1024)\n",
      "\t     max work-items: 256\n",
      "Time for event \"Uploading Buffer A\": 0.020 ms (26787.40 MB/s)\n",
      "Time for event \"Uploading Buffer B\": 0.044 ms (24126.02 MB/s)\n",
      "Time for event \"Running kernel\": 1.448 ms\n",
      "Time for event \"Downloading Buffer C\": 0.074 ms (28828.36 MB/s)\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n"
     ]
    }
   ],
   "source": [
    "!make clean; make mat_mult_profiling.exe; ./mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a4568-4ac0-4def-9b6d-f0ddc7494176",
   "metadata": {},
   "source": [
    "## Open-source profiling tools\n",
    "\n",
    "### Tau\n",
    "\n",
    "[Tau](https://www.cs.uoregon.edu/research/tau/home.php) is a commonly used open-source profiling and tracing toolkit for HPC applications. For OpenCL applications it provides both profiling and tracing functionality.\n",
    "\n",
    "#### Profiling\n",
    "\n",
    "The Tau application **tau_exec** can be used to collect profiling information. Profiling information can then be visualised with the Tau applications **paraprof** (GUI), or **pprof** (command-line).\n",
    "\n",
    "We set the environment variables **PROFILEDIR=./tau** to tell Tau where to put files.\n",
    "\n",
    "```bash\n",
    "export PROFILEDIR=./tau\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7832d88-f179-467a-a1ee-edb71269b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROFILEDIR=./tau\n"
     ]
    }
   ],
   "source": [
    "%env PROFILEDIR=./tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5afba1d-97d2-45b1-9f6b-d73b91b72fee",
   "metadata": {},
   "source": [
    "Then we use the following call to **tau_exec** to collect profiling information for opencl calls. The **-serial** flag is for non-mpi applications, and the **-opencl** flag instructs tau to collect on OpenCL calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "690edd47-7197-4853-a63a-ab9ad86bd26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t               name: gfx1035 \n",
      "\t global memory size: 536 MB\n",
      "\t    max buffer size: 456 MB\n",
      "\t     max local size: (1024,1024,1024)\n",
      "\t     max work-items: 256\n",
      "device id: -2094813872.\n",
      "command id: 94585970317664.\n",
      "vendor id: 0.\n",
      "Got a bogus start! 2 .TAU application\n",
      "Time for event \"Uploading Buffer A\": 0.031 ms (17391.65 MB/s)\n",
      "Time for event \"Uploading Buffer B\": 0.056 ms (19011.75 MB/s)\n",
      "Time for event \"Running kernel\": 2.296 ms\n",
      "Time for event \"Downloading Buffer C\": 0.076 ms (28385.02 MB/s)\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n"
     ]
    }
   ],
   "source": [
    "!tau_exec -T serial -opencl ./mat_mult_profiling.exe -gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9338d-2708-4b49-99c8-90402390ffeb",
   "metadata": {},
   "source": [
    "Now have a look at the contents of the tau directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d759662c-caf0-4a14-b1c6-372512ba1ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events.0.edf   profile.0.0.2  tautrace.0.0.0.trc  tau.trc\n",
      "profile.0.0.0  profile.txt    tautrace.0.0.1.trc  trace.json\n",
      "profile.0.0.1  tau.edf\t      tautrace.0.0.2.trc\n"
     ]
    }
   ],
   "source": [
    "!ls ./tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e657c280-402b-434a-bb02-5aa1599fe54e",
   "metadata": {},
   "source": [
    "Use the Tau application **pprof** to get a text mode profile of the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc64ec07-40a3-4fc4-88e0-530a95acf3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pprof > ./tau/profile.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adde27b-0d4f-440d-a042-bc8792ec5da3",
   "metadata": {},
   "source": [
    "We see from the profile that the call to **mat_mult** took approximately **12ms**. This is similar to what was measured from the profiling interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c5862-a647-4d86-909d-2522b8852954",
   "metadata": {},
   "source": [
    "#### Tracing with Perfetto\n",
    "\n",
    "For tracing we set the environment variables **TRACEDIR=./tau** **TAU_TRACE=1**.\n",
    "\n",
    "```bash\n",
    "export TAU_TRACE=1\n",
    "export TRACEDIR=./tau\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a356019-c905-49c6-991e-ad2298b4d647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TAU_TRACE=1\n",
      "env: TRACEDIR=./tau\n"
     ]
    }
   ],
   "source": [
    "%env TAU_TRACE=1\n",
    "%env TRACEDIR=./tau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f759ad7a-189f-4040-abcd-ae34777a5372",
   "metadata": {},
   "source": [
    "Capture OpenCL information as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c6b2ec1-47c5-4990-928c-be2ca33707e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t               name: gfx1035 \n",
      "\t global memory size: 536 MB\n",
      "\t    max buffer size: 456 MB\n",
      "\t     max local size: (1024,1024,1024)\n",
      "\t     max work-items: 256\n",
      "device id: 1324989776.\n",
      "command id: 93919375223136.\n",
      "vendor id: 0.\n",
      "Got a bogus start! 2 .TAU application\n",
      "Time for event \"Uploading Buffer A\": 0.031 ms (17122.09 MB/s)\n",
      "Time for event \"Uploading Buffer B\": 0.064 ms (16409.19 MB/s)\n",
      "Time for event \"Running kernel\": 2.287 ms\n",
      "Time for event \"Downloading Buffer C\": 0.078 ms (27552.85 MB/s)\n",
      "Maximum error (infinity norm) is: 2.28882e-05\n"
     ]
    }
   ],
   "source": [
    "!tau_exec -T serial -opencl ./mat_mult_profiling.exe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa4a15-cd9f-4fe7-a327-c13ed2fc8a58",
   "metadata": {},
   "source": [
    "Now merge the trace into a downloadable JSON document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef2b39c5-095f-4a1d-a5bb-5f1f832c2923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/tau/2.31.1/x86_64/bin/tau_merge -m tau.edf -e events.0.edf events.0.edf events.0.edf tautrace.0.0.0.trc tautrace.0.0.1.trc tautrace.0.0.2.trc tau.trc\n",
      "tau.trc exists; override [y]? tautrace.0.0.0.trc: 450 records read.\n",
      "tautrace.0.0.1.trc: 6 records read.\n",
      "tautrace.0.0.2.trc: 41 records read.\n"
     ]
    }
   ],
   "source": [
    "!cd tau; echo 'y' | tau_treemerge.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d8aa57a-1fc3-4881-9214-46f172cdb212",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd tau; tau_trace2json ./tau.trc ./tau.edf -chrome -ignoreatomic -o trace.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f69b24f-2aa5-45ba-82de-a08138bb4bb7",
   "metadata": {},
   "source": [
    "Download the file **./tau/trace.json** to your computer. Then in your browser you can go to the address [https://ui.perfetto.dev](https://ui.perfetto.dev) and load the trace for viewing on your local machine. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214adcb4-922f-4b26-b426-76207b50f0aa",
   "metadata": {},
   "source": [
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/Chrome_trace.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Tracing OpenCL calls with <a href=\"https://ui.perfetto.dev\">ui.perfetto.dev</a>.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65792d-d382-4dfb-8dbb-484d861fa839",
   "metadata": {},
   "source": [
    "In this instance we see that the kernel **mat_mult** has taken approximately 0.45ms to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c769e69-47de-4514-8c95-0cec4302a0df",
   "metadata": {},
   "source": [
    "## Commercial profiling tools\n",
    "\n",
    "### CLTracer\n",
    "\n",
    "[CLTracer](https://www.cltracer.com/) is a commerical product that profiles OpenCL calls for Windows and Linux OpenCL applications. It requires a GUI to run and provides \n",
    "\n",
    "* A timeline of OpenCL calls, separated into API and kernel calls\n",
    "* Tables of time spent in each call\n",
    "    * Global and local kernel size recorded\n",
    "    * Size of transfers recorded\n",
    "* Time spent in the API vs time spent blocking\n",
    "* Breakdown of time spent in kernels\n",
    "* Breakdown of time spent in queues\n",
    "\n",
    "Setting up project settings and running a trace for [mat_mult_profiling.cpp](mat_mult_profiling.cpp) was really easy. Unfortunately I don't see the ability to fetch information from the command line.\n",
    "\n",
    "#### Timeline view\n",
    "\n",
    "The timeline view shows when and for how long each OpenCL call lasts. The overview shows that creating the command queues was the most time-consuming operation in the application.\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/cltracer_timeline_overview.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">CLTracer timeline overview.</figcaption>\n",
    "</figure>\n",
    "\n",
    "If we zoom in to the kernel region we see that executing the kernel only took around 2.9ms and that it takes less time to upload and download arrays than the kernel spends executing.\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/cltracer_timeline_zoom.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">CLTracer timeline, zoomed in on kernel region.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eedb5c-6b8c-4319-bad5-314a2f25cfe8",
   "metadata": {},
   "source": [
    "#### Tables\n",
    "\n",
    "The timeline can also be viewed in tabular format. In addition to times for each OpenCL call you can see global and local sizes of the kernels as well as the size of kernel uploads and downloads.\n",
    "\n",
    "<figure style=\"margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/cltracer_tables.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">CLTracer table of OpenCL calls.</figcaption>\n",
    "</figure>\n",
    "\n",
    "For more information on available tools within CLTracer please see the CLTracer [Documentation](https://www.cltracer.com/docs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29020653-17d9-4df0-bc35-81efb9c8f66c",
   "metadata": {},
   "source": [
    "## Vendor profiling tools\n",
    "\n",
    "### AMD\n",
    "\n",
    "#### HSA application traces\n",
    "\n",
    "The AMD utility **rocprof** has the ability to collect traces. We use the **--hsa-trace** option to collect statistics from the underlying HSA calls that provide the OpenCL implementation on AMD.\n",
    "\n",
    "```bash\n",
    "rocprof --hsa-trace -o rocprof_trace/result.csv ./mat_mult_profiling.exe -gpu\n",
    "```\n",
    "\n",
    "Inside the **rocprof_trace** folder you will find the following files:\n",
    "\n",
    "| file | purpose |\n",
    "| --- | --- |\n",
    "| result.sysinfo.txt | System information on available devices |\n",
    "| result.copy_stats.csv | Statistics on all IO calls |\n",
    "| result.hsa_stats.csv | Statistics on HSA function calls |\n",
    "| result.stats.csv | Statistics on all kernel calls |\n",
    "| result.db | SQLITE3 database of profiling information |\n",
    "| result.json | Trace information in JSON format |\n",
    "| result.csv | Information on kernels such as **mat_mult** |\n",
    "\n",
    "We can load the trace file **result.json** using Perfetto, available at [https://ui.perfetto.dev/](https://ui.perfetto.dev/). Copy the trace file **rocprof_trace/result.json** back to your computer and open it with the Perfetto UI. \n",
    "\n",
    "If you zoom (using the `wasd` keys) in you can see calls in GPU threads, COPY threads and HOST threads on the CPU. If you click on the **mat_mult.kd** function you can see how long the kernel took to execute.\n",
    "\n",
    "<figure style=\"margin-left:0; margin-right:auto; width:100%;\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/Perfetto_UI_kernel.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Determining the time for a kernel call</figcaption>\n",
    "</figure>\n",
    "\n",
    "This is a useful method to see how long a kernel execution took, however other OpenCL calls are not directly visible.\n",
    "\n",
    "#### Hardware performance counters with rocprof\n",
    "\n",
    "Hardware performance counters are devices in a processor that measure events, such as the number of wavefronts executed, or the number of times a cache is missed. Rocprof can collect performance counters on kernels. The type of performance counter information that can be captured is obtained with this command:\n",
    "\n",
    "```bash\n",
    "rocprof --list-derived\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66190897-a149-4a1a-8641-2f854f6f6063",
   "metadata": {},
   "source": [
    "We can specify the counters to collect in a file such as [rocprof_counters.txt](rocprof_counters.txt). Here we specify some commonly used metrics for collection. Each **pmc** line is a unique experiment involving an individual run of the code. In this example we collect stats for the **mat_mult** kernel for the first 64 work-items on GPU 0.\n",
    "\n",
    "```txt\n",
    "# Cache hits and Cache misses\n",
    "pmc: TCC_HIT_sum, TCC_MISS_sum\n",
    "\n",
    "# Total video memory fetched and written\n",
    "pmc: FETCH_SIZE, WRITE_SIZE\n",
    "\n",
    "# Percentage of time the GPU was busy, total wavefronts executed\n",
    "pmc: GPUBusy, Wavefronts\n",
    "\n",
    "# Average number of vector and scalar instructions executed per work-item\n",
    "pmc: VALUInsts, SALUInsts\n",
    "\n",
    "# Average number of vector and scalar fetch instructions per work-item\n",
    "pmc: VFetchInsts, SFetchInsts\n",
    "\n",
    "# Average number of vector write instructions per work-item\n",
    "pmc: VWriteInsts\n",
    "\n",
    "# Average number of shared and global memory read or write instructions per work item\n",
    "pmc: LDSInsts, GDSInsts\n",
    "\n",
    "# Percentage of active vector ALU threads in a wave, percentage of GPU time vector and scalar instructions are processed\n",
    "pmc: VALUUtilization, VALUBusy, SALUBusy, \n",
    "\n",
    "# Percentage of fetch, write, atomic, and other instructions that hit the L2 cache\n",
    "pmc: L2CacheHit\n",
    "\n",
    "# Percentage of time the memory unit is active (including stalled), and just stalled, percentage of time the write unit is stalled\n",
    "pmc: MemUnitBusy, MemUnitStalled, WriteUnitStalled\n",
    "\n",
    "# Percentage of time ALU's are stalled by shared memory access, percentage of GPU time local memory is stalled by bank conflicts\n",
    "pmc: ALUStalledByLDS, LDSBankConflict\n",
    "\n",
    "# Dispatches range, which work-items to profile\n",
    "range: 0 : 64\n",
    "# Which GPU's to profile\n",
    "gpu: 0\n",
    "# Names of kernels to profile\n",
    "kernel: mat_mult\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3ab4d0-ecf8-41ef-9574-61321333af8d",
   "metadata": {},
   "source": [
    "Then we can use rocprof to collect the data for these counters.\n",
    "\n",
    "```bash\n",
    "rocprof -i rocprof_counters.txt --timestamp on --stats -o rocprof_counters/result.csv ./mat_mult_profiling.exe -gpu\n",
    "```\n",
    "\n",
    "If your chosen performance counters are supported, then the file [rocprof_counters/result.csv](rocprof_counters/result.csv) should contain a count for every time the counter was triggered. The file [rocprof_counters/example.csv](rocprof_counters/example.csv) is an example file collected with rocprof on **mat_mult_profiling.exe**. This [page](https://docs.amd.com/bundle/ROCProfiler-User-Guide-v5.1/page/rocprof_Command_Line_Tool.html) has information on what the keys in the CSV file mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9386a4-12ff-44b2-9de0-f320014dc917",
   "metadata": {},
   "source": [
    "#### Rocprof under a job manager\n",
    "\n",
    "Rocprof runs fine under a job manager like SLURM, you just need to make an output file for each process launched. For example on SLURM the `$SLURM_JOBID` and `$SLURM_PROCID` environment variables are helpful in separating the output. Put the rocprof commands in a script called **profile.sh**.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "rocprof -i rocprof_counters.txt -o rocprof_counters/result-$SLURM_JOBID-$SLURM_PROCID.csv ./mat_mult_profiling_mpi.exe\n",
    "```\n",
    "\n",
    "Then you can run the script from **srun** like this so it picks up the environment variable **$SLURM_PROCID** from within the script.\n",
    "\n",
    "```bash\n",
    "srun -N $SLURM_JOB_NUM_NODES -n $SLURM_NTASKS -c $OMP_NUM_THREADS ./profile.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f8873-6b6a-4bf1-8c96-86057d9559b4",
   "metadata": {},
   "source": [
    "A complete example for using rocprof with an MPI-enabled application is in **course_material/L2_Using_OpenCL_On_Setonix/rocprof_mpi**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9342ee5a-80fc-4176-93d3-85f4c5fde70f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NVIDIA\n",
    "\n",
    "Historically there was limited functionality for profiling OpenCL events with NVIDIA's [NVVP](http://uob-hpc.github.io/2015/05/27/nvvp-import-opencl.html), however profiling support for OpenCL has largely disappeared, with the implementation of NSight Compute and NSight Systems.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c38c04c-7c8d-4646-a382-0b02fb91a42e",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a> for the <a href=\"https://pawsey.org.au\">Pawsey Supercomputing Research Centre</a>. All trademarks mentioned are the property of their respective owners.\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
