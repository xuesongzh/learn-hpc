{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4710962-c1a2-40a2-ac38-878dd75378b5",
   "metadata": {},
   "source": [
    "# Using OpenCL on Setonix\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Setonix is a world class supercomputer, delivering over 27 Petaflops of floating point performance using AMD EPYC CPUs and Instinct MI250x GPUs. As of November 2022 Setonix sits in place 15 on the [TOP 500](https://top500.org/system/180123/) list of the world's most powerful supercomputers and number 4 of the [Green 500](https://www.top500.org/lists/green500/2022/11/) at 57 GigaFLOPS/Watt.\n",
    "\n",
    "## Official documentation\n",
    "\n",
    "The [Pawsey Documentation Portal](https://support.pawsey.org.au/documentation/) should be your first point of call when looking for documentation. That source **must take priority** if there is any discrepancy between the official documentation and this material. On  this [page](https://support.pawsey.org.au/documentation/display/US/Setonix+GPU+Partition+Quick+Start) is some specific documentation for using GPU's on Setonix. \n",
    "\n",
    "## Access to Setonix\n",
    "\n",
    "Firstly, you need a username and password to access Setonix. Your **username** and **password** will be given to you prior to the beginning of this workshop. If you are using your regular Pawsey account then you can reset your password [here](https://support.pawsey.org.au/password-reset/).\n",
    "\n",
    "Access to Setonix is via Secure SHell (SSH). On Linux, Mac OS, and Windows 10 and higher an SSH client is available from the command line or terminal application. Otherwise you need to use a client program like [Putty](https://www.putty.org/) or [MobaXterm](https://mobaxterm.mobatek.net/download-home-edition.html).\n",
    "\n",
    "### Access with SSH on the command line\n",
    "\n",
    "On the command line use **ssh** to access Setonix.\n",
    "\n",
    "```bash\n",
    "ssh -Y <username>@setonix.pawsey.org.au\n",
    "```\n",
    "\n",
    "#### Passwordless login with SSH\n",
    "\n",
    "In order to avoid specifying a username and password on each login you can generate a keypair on your computer, like this:\n",
    "\n",
    "```bash\n",
    "ssh-keygen -t rsa\n",
    "```\n",
    "\n",
    "Then copy the public key (the file that ends in \\*.pub) to your account on Setonix and append it to the authorized_keys file in `${HOME}/.ssh`. On your machine run this command:\n",
    "\n",
    "```bash\n",
    "scp -r <filename>.pub <username>@setonix.pawsey.org.au\n",
    "```\n",
    "\n",
    "Then login to Setonix and run this command\n",
    "\n",
    "```bash\n",
    "mkdir -p ${HOME}/.ssh\n",
    "cat <filename>.pub >> ${HOME}/.ssh/authorized_keys\n",
    "chmod -R 0400 ${HOME}/.ssh\n",
    "```\n",
    "\n",
    "Then you can run \n",
    "\n",
    "```bash\n",
    "ssh <username>@setonix.pawsey.org.au\n",
    "```\n",
    "\n",
    "without a password.\n",
    "\n",
    "### Access from Windows with the MobaXterm client\n",
    "\n",
    "If you have a OS that is older than Windows 10, and need a client in a hurry, then just download **MobaXterm Home (Portable Edition)** from [this location](https://mobaxterm.mobatek.net/download-home-edition.html). Extract the Zip file and run the application. You might need to accept a firewall notification. \n",
    "\n",
    "Now go to **Settings -> SSH** and uncheck **\"Enable graphical SSH-browser\"** in the SSH-browser settings pane. Also enable **\"SSH keepalive\"** to keep SSH connections active.\n",
    "\n",
    "<figure style=\"margin-bottom 3em; margin-top: 2em; margin-left:auto; margin-right:auto; width:100%\">\n",
    "    <img style=\"vertical-align:middle\" src=\"../images/MobaXTerm_Settings.svg\"> <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Figure: MobaXTerm settings.</figcaption>\n",
    "</figure>\n",
    "\n",
    "Close the MobaXTerm settings and start a local terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed4b21-7b64-4fd2-b25c-7ac25d60a571",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Hardware environment on Setonix\n",
    "\n",
    "On Setonix there are two main kinds of compute nodes:\n",
    "\n",
    "* CPU nodes with 2 sockets and 128 cores, 256 threads.\n",
    "* GPU nodes with 1 CPU socket with 64 cores, 128 threads, and 4 MI250X GPU sockets. Each MI250X GPU socket has two GPU compute devices.\n",
    "\n",
    "### CPU nodes\n",
    "\n",
    "CPU nodes are based on the AMD<span>&trade;</span> EPYC<span>&trade;</span> 7763 processor in a dual-socket configuration. Each processor has a multi-chip design with 8 chiplets (Core CompleX's). Shown below is a near infrared image of an EPYC processor, showing 8 chiplets and an IO die. \n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:50%;\">\n",
    "    <img src=\"images/EPYC_7702_delidded.jpg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Near infrared photograph of a de-lidded AMD EPYC CPU with chiplets and IO die. Image credit: <a href=\"https://commons.wikimedia.org/wiki/File:AMD_Epyc_7702_delidded.jpg\")>Wikipedia.</a> </figcaption>\n",
    "</figure>\n",
    "\n",
    "Each chiplet has 8 cores, and these cores share access to a 32 MB L3 cache. Every core has its own L1 and L2 cache, provides 2 hardware threads, and has access to SIMD units that can perform floating point math on vectors up to 256 bits (8x32-bit floats) wide in a single clock cycle. There are 16 hardware threads available per chiplet. Since every processor has 8 chiplets, there are a total of 64 cores 128 threads per processor; and 128 cores 256 threads per node. Here is some cache and performance information for the AMD Epyc 7763 CPU.\n",
    "\n",
    "| Node | CPU | Base clock freq(GHz) | Peak clock freq (GHz) | Cores | Hardware threads | L1 Cache (KB) | L2 Cache (KB) | L3 cache (MB) | FP SIMD width (bits) | Peak TFLOPs (FP32) |\n",
    "|:----:|:----:|-----:| -----: | -----: | :----: | :----: | :----: | :----: | :----: | :---: |\n",
    "| CPU |AMD EPYC 7763 | 2.45 | 3.50 | 64 | 128 | 64x32 | 64x512 | 8x32 | 256 | ~1.79 |\n",
    "\n",
    "Below is an image of a CPU compute blade on Setonix, in this shot there are 8 CPU heatsinks for a total of four nodes per blade.  \n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img src=\"images/cpu_blade.jpg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">A CPU blade on Setonix, showing four compute nodes per blade. Each compute node has two CPU sockets.</figcaption>\n",
    "</figure>\n",
    "\n",
    "### GPU nodes\n",
    "\n",
    "GPU nodes on Setonix have **one** AMD 7A53 'Trento' CPU processor and **four** MI250X GPU processors. The CPU is a specially-optimized version of the EPYC processor used in the CPU nodes, but otherwise has the same design and architecture. The Instinct<span>&trade;</span> MI250X processor is also a Multi-Chip Module (MCM) design, with two graphics dies (otherwise known as Graphics Complex Dies) that provide two GPU compute devices per processor, as shown below.\n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img src=\"../images/MI250x.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">AMD Instinct<span>&trade;</span> MI250X compute architecture, showing two GPU devices per processor. Image credit: <a href=\"https://hc34.hotchips.org/\")>AMD Instinct<span>&trade;</span> MI200 Series Accelerator and Node Architectures | Hot Chips 34</a></figcaption>\n",
    "</figure>\n",
    "\n",
    "Each of the two Graphics Compute Dies (GCD's) in a MI250X appears to OpenCL as a **individual compute device** with its own 64 GB of global memory and 8MB of L2 cache. Since there are four MI250X's, **there are a total of 8 GPU compute devices visible to OpenCL per GPU node**. The compute devices have 110 **compute units**, and each compute unit executes instructions over a bank of 4x16 floating point SIMD units that share a 16KB L1 cache, as seen below:\n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img src=\"images/Setonix-GPU-Compute-Unit.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Close-up of an AMD Instinct MI250X compute unit.</figcaption>\n",
    "</figure>\n",
    "\n",
    "The interesting thing to note with these compute units is that both 64-bit and 32-bit floating instructions are executed natively **at the same rate**. Therefore only the increased bandwidth requirements for moving 64-bit numbers around is a performance consideration. Below is a table of performance numbers for each of the four dual-gpu MI250X processors in a gpu node.\n",
    "\n",
    "| Card | Boost clock (GHz)| Compute Units | FP32 Processing Elements | FP64 Processing Elements (equivalent compute capacity) | L1 Cache (KB) | L2 Cache (MB) | device memory (GB) | Peak Tflops (FP32)| Peak Tflops (FP64)|\n",
    "|:----:|:-----| :----- | :----- | :---- | :---- | :---- | :---- | :---- | :---- |\n",
    "| AMD Radeon Instinct MI250x |1.7 | 2x110 | 2x7040 | 2x7040 | 2x110x16 | 2x8 | 2x64 | 47.9 | 47.9 |\n",
    "\n",
    "Below is an installation image of a GPU compute blade with two nodes. Each node has 1 CPU socket and four GPU sockets.\n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img src=\"images/gpu_blade.jpg\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">A GPU blade on Setonix, showing two GPU nodes, each node has one CPU socket and four GPU sockets.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941ec5e-e17a-47a5-9138-8bdc2fb3c523",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Job queues\n",
    "\n",
    "On Setonix the following queues are available for general use. A special account is needed to access the `gpu` queue. This will usually be your project name followed by the suffix **-gpu**.\n",
    "\n",
    "|Queue| Max time limit| Processing elements (CPU) | Socket| Cores| processing elements per CPU core | Available memory (GB) | Number of OpenCL devices | Memory per OpenCL device (GB) |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| work | 24 hours | 256 | 2 | 64 | 2 | ~230 | 1 | ~230 |\n",
    "| long | 96 hours | 256 | 2 | 64 | 2 | ~230 | 1 | ~230 |\n",
    "| debug | 1 hour | 256 | 2 | 64 | 2 | ~230 | 1 | ~230 |\n",
    "| highmem | 96 hours | 256 | 2 | 64 | 2 | ~980 | 1 | ~980 |\n",
    "| copy | 24 hours | 32 | 1 | 64 | 2 | ~118 | 1 | ~118 |\n",
    "| gpu | 24 hours | 128 | 1 | 64 | 2 | ~230 | 4x2 | 64 |\n",
    "| gpu-highmem | 24 hours | 128 | 1 | 64 | 2 | ~460 | 4x2 | 64 |\n",
    "| gpu-dev | 4 hours | 128 | 1 | 64 | 2 | ~230 | 4x2 | 64 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca2784-aecf-4496-8c93-d546565bfde3",
   "metadata": {},
   "source": [
    "## Interactive jobs\n",
    "\n",
    "When compiling software on Setonix it is good practice to compile on a compute node. The following commands help you get an interactive job on either a CPU node or a GPU node. These are listed for information purposes. For the workshop we will use the `salloc` command given in the welcome letter.\n",
    "\n",
    "### Interactive jobs on CPU nodes\n",
    "\n",
    "The **work** queue is the queue to use for applications that run exclusively on a CPU node. You can use the following command to get an interactive job that has one MPI process with access to 8 OpenMP threads.\n",
    "\n",
    "```bash\n",
    "salloc --account=${PAWSEY_PROJECT} --ntasks=1 --mem=8GB --cpus-per-task=8 --time=4:00:00 --partition=work\n",
    "```\n",
    "\n",
    "### Interactive jobs on GPU nodes\n",
    "\n",
    "Allocations for the **gpu** queue on Setonix need a separate account with the **-gpu** suffix. The following command reserves 1 MPI process with access to 8 OpenMP threads and one GPU (one GCD) for interactive use.             \n",
    "\n",
    "```bash\n",
    "salloc --account=${PAWSEY_PROJECT}-gpu --ntasks=1 --mem=8GB --cpus-per-task=8 --time=4:00:00 --gpus-per-task=1 --partition=gpu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484c6de0-d2f6-4e65-b207-bf9f16686a04",
   "metadata": {},
   "source": [
    "## Building software for Setonix\n",
    "\n",
    "OpenCL is different from CUDA and HIP in that compilation of kernels is done **within the library of an OpenCL implementation** rather than by a vendor-specific compiler. This means that you are free to use whatever programming environment is most suitable for your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00304c87-b992-4918-8d0b-83532c5b8f69",
   "metadata": {},
   "source": [
    "### Software modules\n",
    "\n",
    "#### Programming environment\n",
    "\n",
    "There are three main programming environments available on Setonix. Each provides C/C++ and Fortran compilers that build software with knowledge of of the MPI libraries available on Setonix. The **PrgEnv-GNU** programming environment loads the GNU compilers for best software compatibility, the module **PrgEnv-aocc** loads the AMD **aocc** optimising compiler to try and get the best performance from the AMD CPU's on Setonix, and the **PrgEnv-cray** environment loads the well-supported compilers from Cray. Use these commands to find which module to load.\n",
    "\n",
    "| Programming environment | command to use |\n",
    "| :--- | :--- |\n",
    "| AMD | ```module avail PrgEnv-aocc``` |\n",
    "| Cray | ```module avail PrgEnv-cray``` |\n",
    "| GNU | ```module avail PrgEnv-gnu``` |\n",
    "\n",
    "Then the following compiler wrappers are available for use to compile source files:\n",
    "\n",
    "| Command | Explanation |\n",
    "| :--- | :--- |\n",
    "| cc | C compiler |\n",
    "| CC | C++ compiler |\n",
    "| ftn | FORTRAN compiler |\n",
    "\n",
    "In order to use a GPU-aware MPI library from Cray you also need to load the **craype-accel-amd-gfx90a** module, which is available in all three programming environments.  Load the module with this command.\n",
    "\n",
    "```bash\n",
    "module load craype-accel-amd-gfx90a\n",
    "```\n",
    "\n",
    "then set this environment variable to enable GPU support with MPI.\n",
    "\n",
    "```bash\n",
    "export MPICH_GPU_SUPPORT_ENABLED=1\n",
    "```\n",
    "\n",
    "#### ROCm\n",
    "\n",
    "The ROCm library from AMD provides both an OpenCL implementation as well as AMD tools like profilers. You can load the ROCm library with this command:\n",
    "\n",
    "```bash\n",
    "module load rocm/5.4.3\n",
    "```\n",
    "\n",
    "#### Custom OpenCL environment\n",
    "\n",
    "OpenCL support comes with the ROCm module, however the OpenCL header and ICD loader that somes with ROCm is quite old, not using the latest OpenCL API. There is an OpenCL environment that has been put together specifically for this course. It uses the [Portable OpenCL library](http://portablecl.org/) library to utilise CPU's as compute devices, loads the ROCm module, and provides access to OpenCL tools and the latest headers and ICD loader from Khronos. Use these commands to load this environment:\n",
    "\n",
    "```bash\n",
    "module use /software/projects/courses01/setonix/opencl/modulefiles\n",
    "module load PrgEnv-opencl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c83b28-47b7-41c4-8850-33870c9886ff",
   "metadata": {},
   "source": [
    "#### Omnitrace support\n",
    "\n",
    "[Omnitrace](https://github.com/AMDResearch/omnitrace) is a tool for using rocprof to collect **traces**, or information on **when** an application component starts using compute resources, and **for how long** it uses those resources. Currently you will need these modules loaded to access the experimental Omnitrace tools.\n",
    "\n",
    "```bash\n",
    "module load rocm/5.0.2\n",
    "module use /software/projects/courses01/setonix/omnitrace/share/modulefiles\n",
    "module load omnitrace/1.10.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d143c-6d94-4a2f-b908-9f7268b667c1",
   "metadata": {},
   "source": [
    "#### Omniperf support\n",
    "\n",
    "[Omniperf](https://github.com/AMDResearch/omniperf) is a tool to make low level information collected by **rocprof** accessible. It can perform feats like creating [roofline models](https://en.wikipedia.org/wiki/Roofline_model) of how well your kernels are performing, in relation to the theoretical capability of the compute hardware. The following commands will help you access the experimental Omniperf tools.\n",
    "\n",
    "```bash\n",
    "module load cray-python\n",
    "module load rocm/5.0.2\n",
    "module use /software/projects/courses01/setonix/omniperf/1.0.8PR2/modulefiles\n",
    "module load omniperf/1.0.8-PR2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ea2ac-377e-4414-9d08-f52384bc08a3",
   "metadata": {},
   "source": [
    "### Compiling software with OpenCL and MPI support\n",
    "\n",
    "You can compile MPI software with OpenCL using the compiler wrapper **CC** from one of the three available programming environments. In order provide the best chance of reducing compiler issues it is **best practice to compile from the compute node** that you are going to use. Here are some suggested compiler flags.\n",
    "\n",
    "| Function | flags |\n",
    "| :--- | :--- |\n",
    "| Production (compile and link) | ```-g -O2``` |\n",
    "| Debug (compile and link) | ```-O0 -g``` |\n",
    "| OpenMP (compile and link)| ```-fopenmp``` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55bf4c7-ffbb-4437-97b4-4796549be5df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise: compile and run your first MPI-enabled OpenCL application\n",
    "\n",
    "In the file [hello_devices_mpi.cpp](hello_devices_mpi.cpp) is a MPI-enabled OpenCL application that reports on devices and fills a vector. Your task is to compile this file into an executable called **hello_devices_mpi.exe**.\n",
    "\n",
    "### Compilation steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbf51d-2546-4fee-a090-7931410a780f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task 1. Login and setup\n",
    "\n",
    "* Log into **setonix.pawsey.org.au**.\n",
    "```bash\n",
    "ssh <username>@setonix.pawsey.org.au\n",
    "```\n",
    "* Change directory to your space on /scratch.\n",
    "```bash\n",
    "cd $MYSCRATCH\n",
    "```\n",
    "* Get the course material from Github if don't already have it.\n",
    "```bash\n",
    "wget https://github.com/pelagos-consulting/OpenCL_Course/archive/refs/heads/main.zip\n",
    "unzip -DD main.zip\n",
    "cd OpenCL_Course-main/course_material/L2_Using_OpenCL_On_Setonix\n",
    "```\n",
    "* Get an interactive GPU job on Setonix. The correct command to use will be in the welcome letter, and looks something like this: \n",
    "```bash\n",
    "salloc --account ${PAWSEY_PROJECT}-gpu --ntasks 1 --mem 8GB --cpus-per-task 8 --time 1:00:00 --gpus-per-task 1 --partition gpu\n",
    "```\n",
    "\n",
    "* Load the ROCm module\n",
    "\n",
    "```bash\n",
    "module load rocm/5.0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b47c9-66ff-4ab6-9897-a4c35fc7d78b",
   "metadata": {
    "tags": []
   },
   "source": [
    "    \n",
    "#### Task 2. Compile the program with the OpenCL headers and ICD loader from ROCm\n",
    "\n",
    "* Compile the file [hello_devices_mpi.cpp](hello_devices_mpi.cpp) with the `CC` compiler wrapper. There is an OpenCL header directory in `/opt/rocm/opencl/include` and an ICD loader in `/opt/rocm/opencl/lib`. Use those to compile the application:\n",
    "\n",
    "```bash\n",
    "CC -g -fopenmp -O2 -I../include -I/opt/rocm/opencl/include -L/opt/rocm/opencl/lib hello_devices_mpi.cpp -o hello_devices_mpi.exe -lOpenCL\n",
    "./hello_devices_mpi.exe\n",
    "```\n",
    "\n",
    "Notice that we had a compiler warning with `CL_TARGET_OPENCL_VERSION`. This is because we are targeting OpenCL 3.0, but the ROCm header libraries aren't aware of the API change. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5e038b-5dae-4fd4-8c10-a48d72d418e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task 3. Compile the program with the OpenCL headers and ICD loader from Khronos\n",
    "\n",
    "There is an OpenCL header directory from Khronos in `/software/projects/courses01/setonix/opencl/OpenCL-Headers/install/include` and an updated ICD loader in `/software/projects/courses01/setonix/opencl/OpenCL-ICD-Loader/install/lib64`. We can also use those to compile.\n",
    "\n",
    "```bash\n",
    "CC -g -fopenmp -O2 -I../include -I/software/projects/courses01/setonix/opencl/OpenCL-Headers/install/include -L/software/projects/courses01/setonix/opencl/OpenCL-ICD-Loader/install/lib64 hello_devices_mpi.cpp -o hello_devices_mpi.exe -lOpenCL\n",
    "./hello_devices_mpi.exe\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d458d-1f61-4195-87ee-a5d067d08eaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Task 4. Use the PrgEnv-opencl module\n",
    "\n",
    "The **PrgEnv-opencl** module from `/software/projects/courses01/setonix/opencl/modulefiles` makes available the OpenCL headers and ICD loader from Khronos. It adds the Khronos header path to the **CPATH** environment variable and the ICD loader path to the **LD_LIBRARY_PATH** and **LIBRARY_PATH** environment variables. Now we can just compile without explicitly specifying the header and ICD loader directories.\n",
    "\n",
    "```bash\n",
    "module use /software/projects/courses01/setonix/opencl/modulefiles\n",
    "module load PrgEnv-opencl\n",
    "CC -g -fopenmp -O2 -I../include hello_devices_mpi.cpp -o hello_devices_mpi.exe -lOpenCL\n",
    "./hello_devices_mpi.exe\n",
    "```\n",
    "\n",
    " The module also adds the [PoCL](http://portablecl.org/) OpenCL implementation so we can also use the CPU as a compute device. This extra implementation was enabled by installing PoCL to `/software/projects/courses01/setonix/opencl/pocl/3.1` and setting the environment variable **OCL_ICD_VENDORS** to `/software/projects/courses01/setonix/opencl/OpenCL_vendors` where a file called `pocl.icd` points to the pocl vendor library in `/software/projects/courses01/setonix/opencl/pocl/3.1/lib64/libpocl.so.2.10.0`. \n",
    " \n",
    "On line 55 of `hello_devices_mpi.cpp` change the type of device to select from: \n",
    " \n",
    "```C++\n",
    "    // Set the target device\n",
    "    cl_device_type target_device=CL_DEVICE_TYPE_GPU;\n",
    "```\n",
    "\n",
    "Change this to\n",
    "\n",
    "```C++\n",
    "    // Set the target device\n",
    "    cl_device_type target_device=CL_DEVICE_TYPE_ALL;\n",
    "```\n",
    "\n",
    "Now recompile and run\n",
    "\n",
    "```bash\n",
    "CC -g -fopenmp -O2 -I../include hello_devices_mpi.cpp -o hello_devices_mpi.exe -lOpenCL\n",
    "./hello_devices_mpi.exe\n",
    "``` \n",
    "\n",
    "Now you should see the CPU as a compute device!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff480a8-30d6-480a-9f35-068ab0551d43",
   "metadata": {},
   "source": [
    "#### Bonus task\n",
    "\n",
    "Try changing the number of GPU's in your request for resources for the interactive job. How many compute devices appear in the output from the above command?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea2932-843d-443c-8214-0006bc5feef2",
   "metadata": {},
   "source": [
    "#### Makefile solution\n",
    "\n",
    "If you get stuck, the example [Makefile](Makefile) contains the above compilation steps. Assuming you loaded the right modules defined above, the make command is run as follows:\n",
    "\n",
    "```bash\n",
    "make clean; make\n",
    "```\n",
    "\n",
    "The script **run_compile.sh** contains the necessary commands to load the appropriate modules and run the **make** command.\n",
    "\n",
    "```bash\n",
    "chmod 700 run_compile.sh\n",
    "./run_compile.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f3ed9-e47e-4c87-8560-9e0d48fea2d6",
   "metadata": {},
   "source": [
    "## Batch jobs with OpenCL on GPU nodes\n",
    "\n",
    "Pawsey has extensive documentation available for running jobs, at this [site](https://support.pawsey.org.au/documentation/display/US/Running+Jobs+in+Setonix). Here is some information that is specific to making best use of the GPU nodes on Setonix.\n",
    "\n",
    "### GPU node configuration\n",
    "\n",
    "On the GPU nodes of Setonix there is 1 CPU and 8 compute devices. Each of the 8 chiplets in the CPU is intended to have optimal access to one of the 8 available GPU compute devices. Shown below is a hardware diagram of a compute node, where each chiplet is connected optimally to one compute device.\n",
    "\n",
    "<figure style=\"margin: 1em; margin-left:auto; margin-right:auto; width:100%;\">\n",
    "    <img src=\"images/Setonix-GPU-Node.png\">\n",
    "    <figcaption style= \"text-align:lower; margin:1em; float:bottom; vertical-align:bottom;\">Overall view of a Setonix GPU node, showing the placement of hardware threads and the closest available compute device.</figcaption>\n",
    "</figure>\n",
    "\n",
    "From the above diagram we see that best use of the GPU's occur when a chiplet accesses a GPU that is closest to it. Work is still being done on making sure that MPI processes map optimally to available compute devices, however these interim suggestions will help space out the MPI tasks so each task resides on its own chiplet.\n",
    "\n",
    "* Use **--ntasks-per-node=8** to allocate up to 8 MPI tasks per node, one task per chiplet/compute device pair.\n",
    "* Use **--gpus-per-task=1** to allocate 1 compute device per MPI task.\n",
    "* Use **--cpus-per-task=8** and **--threads-per-core=1** to allocate all available threads in a chiplet to a single MPI process.\n",
    "* Use the **--gpu-bind=closest** option to bind each compute device to the closest MPI task.\n",
    "* Use the **--exclusive** option to have exclusive use of all the resources on a node. This will make your job harder to get through the queues, so use this only if you **absolutely** need all the resources on a gpu node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecf7671-53cc-4e97-bb5d-ec6547cca8a8",
   "metadata": {},
   "source": [
    "### Example job script\n",
    "\n",
    "The suggested job script below will allocate an MPI task for every compute device on a node of Setonix. Then it will allocate 8 OpenMP threads to each MPI task. We can use the helper program [hello_jobstep.cpp](hello_jobstep.cpp) adapted from a [program](https://code.ornl.gov/olcf/hello_jobstep) by Thomas Papatheodore from ORNL. Every software thread executed by the program reports the MPI rank, OpenMP thread, the CPU hardware thread, as well as the GPU and BUS ID's of the GPU hardware.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash -l\n",
    "\n",
    "#SBATCH --account=<account>-gpu    # your account\n",
    "#SBATCH --partition=gpu            # Using the gpu partition\n",
    "#SBATCH --ntasks=8                 # Total number of tasks\n",
    "#SBATCH --ntasks-per-node=8        # Set this for 1 mpi task per compute device\n",
    "#SBATCH --cpus-per-task=8          # How many OpenMP threads per MPI task\n",
    "#SBATCH --threads-per-core=1       # How many OpenMP threads per core (1 or 2)\n",
    "#SBATCH --gpus-per-task=1          # How many OpenCL compute devices to allocate to a  task\n",
    "#SBATCH --gpu-bind=closest         # Bind each MPI taks to the nearest GPU\n",
    "#SBATCH --mem=4000M                # Indicate the amount of memory per node when asking for shared resources\n",
    "#SBATCH --time=00:05:00            # Estimated time in HH:MM:SS\n",
    "\n",
    "module use /software/projects/courses01/setonix/opencl/modulefiles\n",
    "module load PrgEnv-opencl\n",
    "module load craype-accel-amd-gfx90a\n",
    "\n",
    "# Recompile the software\n",
    "make clean; make\n",
    "\n",
    "export MPICH_GPU_SUPPORT_ENABLED=1 # Enable GPU support with MPI\n",
    "\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK   #To define the number of OpenMP threads available per MPI task, in this case it will be 8\n",
    "export OMP_PLACES=cores     #To bind to cores \n",
    "export OMP_PROC_BIND=close  #To bind (fix) threads (allocating them as close as possible). This option works together with the \"places\" indicated above, then: allocates threads in closest cores.\n",
    " \n",
    "# Temporal workaround for avoiding Slingshot issues on shared nodes:\n",
    "export FI_CXI_DEFAULT_VNI=$(od -vAn -N4 -tu < /dev/urandom)\n",
    "\n",
    "# Run a job with task placement and $BIND_OPTIONS\n",
    "#srun -N $SLURM_JOB_NUM_NODES -n $SLURM_NTASKS -c $OMP_NUM_THREADS $BIND_OPTIONS  ./hello_jobstep.exe\n",
    "srun -N $SLURM_JOB_NUM_NODES -n $SLURM_NTASKS -c $OMP_NUM_THREADS ./hello_jobstep.exe | sort\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565a531-f16b-4e7f-ba19-e8fcf1387da8",
   "metadata": {},
   "source": [
    "In the file [jobscript.sh](jobscript.sh) is a batch script for the information above. Edit the `<account>` field to include the account to charge to. The value to use will be in the environment variable `$PAWSEY_PROJECT`. \n",
    "\n",
    "```bash\n",
    "echo $PAWSEY_PROJECT\n",
    "```\n",
    "\n",
    "Then submit the script to the batch queue with this command\n",
    "\n",
    "```bash\n",
    "sbatch jobscript.sh\n",
    "```\n",
    "\n",
    "Use this command to check on the progress of your job\n",
    "\n",
    "```bash\n",
    "squeue --me\n",
    "```\n",
    "\n",
    "Then if you need to you and you know the job id you can cancel a job with this command\n",
    "\n",
    "```bash\n",
    "scancel <jobID>\n",
    "```\n",
    "\n",
    "Once the job is done, have a look at the `*.out` file and examine how the threads and GPU's are placed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca781fb0-62c1-4b44-9e07-9ca11547a05b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section we cover using OpenCL on the Pawsey Supercomputer Setonix. This includes logins with SSH;  hardware and software environments; and accessing the job queues through interactive and batch jobs. We conclude the chapter with the OpenCL software compilation process on Setonix, and then how to get the best performance in batch jobs by scheduling MPI tasks close to the available compute devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4576ef5-cae8-48db-9c93-acf6009975bf",
   "metadata": {},
   "source": [
    "<address>\n",
    "Written by Dr. Toby Potter of <a href=\"https://www.pelagos-consulting.com\">Pelagos Consulting and Education</a>, for the <a href=\"https://pawsey.org.au\">Pawsey Supercomputing Research Centre</a>, and with contributions from the Pawsey team. All trademarks mentioned in this teaching series belong to their respective owners.\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
