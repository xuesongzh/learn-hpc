{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing OpenCL kernels\n",
    "\n",
    "Measuring the performance of OpenCL kernels is important for finding out how well they peform. The OpenCL framework provides a way to get profiling information from kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-use the matrix multiplication code for this example. One optimisation that can be applied is to transpose the matrix before using it in the matrix computation as shown in the following figure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure style=\"float:center\">\n",
    "    <img style=\"display:inline-block; vertical-align:top; margin:20px\" src=\"images/transposed_matrix.png\" width=\"95%\">\n",
    "    <figcaption style= \"text-align:lower; margin:2px; float:bottom; vertical-align:bottom\">Figure: Matrix transpose as a potential optimisation </figcaption>\n",
    "</figure>\n",
    "\n",
    "The theory behind this is that since memory elements in dimension 0 (the row dimension) are contiguous, memory accesses to and from matrix $A$ will be more efficient if $A$ was transposed ($A^{T}$) prior to computing the matrix multiplication. Then an element of matrix C at position $(m,n)$ is computed as the dot product of column $m$ from matrix $A^T$ and column $n$ from matrix $B$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code [mat_mult_transpose](code/mat_mult_transpose.cpp) has been modified to include two additional kernels that transpose matrix A and use it in the matrix multiplication. It uses the same data as the simple matrix multiplication code, however we regenerate the data here for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Code to make the test file\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "nrows=1024\n",
    "ncols=1024\n",
    "\n",
    "# Make up some arrays of random numbers\n",
    "matrix_A=np.random.random((nrows, ncols)).astype(np.float32)\n",
    "matrix_B=np.random.random((nrows, ncols)).astype(np.float32)\n",
    "\n",
    "# Make up the answer, the resulting Matrix C\n",
    "matrix_C=np.matmul(matrix_A, matrix_B).astype(np.float32)\n",
    "\n",
    "# Write the files to grid, notice how I unravel in column major (f) format\n",
    "# before writing to file\n",
    "matrix_A.ravel(order=\"f\").tofile(os.path.join(\"code\",\"array_A_1D.dat\"))\n",
    "matrix_B.ravel(order=\"f\").tofile(os.path.join(\"code\",\"array_B_1D.dat\"))\n",
    "matrix_C.ravel(order=\"f\").tofile(os.path.join(\"code\",\"array_C_answer_1D.dat\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After compilation you can just run the code and see how much faster the transposed version is compared to the standard matrix multiply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform 0: Experimental OpenCL 2.1 CPU Only Platform, vendor: Intel(R) Corporation, version OpenCL 2.1 LINUX\n",
      "Platform 1: NVIDIA CUDA, vendor: NVIDIA Corporation, version OpenCL 1.2 CUDA 9.1.83\n",
      "Platform 0 has 1 devices\n",
      "Platform 1 has 0 devices\n",
      "Matrix transpose took 9.201738 ms\n",
      "Standard matrix multiply took 173.220850 ms\n",
      "Transposed matrix multiply took 57.538626 ms\n",
      "Transposed approach resulted in a speedup of 2.595444x\n",
      "RMS difference is 0.000114229\n",
      "Elapsed time is 0.473087seconds\n"
     ]
    }
   ],
   "source": [
    "!cd code; ./mat_mult_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try editing line 77 to switch from CPU devices to GPU devices if you have one available and recompile.\n",
    "\n",
    "from\n",
    "\n",
    "```C\n",
    "cl_device_type target_device=CL_DEVICE_TYPE_CPU;\n",
    "```\n",
    "\n",
    "to\n",
    "\n",
    "```C\n",
    "cl_device_type target_device=CL_DEVICE_TYPE_GPU;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform 0: Experimental OpenCL 2.1 CPU Only Platform, vendor: Intel(R) Corporation, version OpenCL 2.1 LINUX\n",
      "Platform 1: NVIDIA CUDA, vendor: NVIDIA Corporation, version OpenCL 1.2 CUDA 9.1.83\n",
      "Platform 0 has 0 devices\n",
      "Platform 1 has 1 devices\n",
      "Matrix transpose took 0.166912 ms\n",
      "Standard matrix multiply took 18.401280 ms\n",
      "Transposed matrix multiply took 97.909760 ms\n",
      "Transposed approach resulted in a speedup of 0.187621x\n",
      "RMS difference is 0.000114286\n",
      "Elapsed time is 0.492796seconds\n"
     ]
    }
   ],
   "source": [
    "!cd code; ./mat_mult_transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my machine and a GPU it turns out that the transposed matrix calculation is actually around 5x *slower* than the standard implementation. If I use the CPU then the transposed matrix approach is around 2.6x *faster*. I am not certain why the code on the GPU is slower, however being able to find this information out is very helpful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machinery to time kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to time kernels, a few key changes must be made to the standard matrix multiplication code. Firstly, the command queue has to be enabled to measure profiling events. This is achieved by setting the flag **CL_QUEUE_PROFILING_ENABLE** in the call to the [clCreateCommandQueue](https://www.khronos.org/registry/OpenCL/sdk/1.2/docs/man/xhtml/clCreateCommandQueue.html) function when the command queues are created. \n",
    "\n",
    "```C\n",
    "command_queues_1d[queue_counter]=clCreateCommandQueue(  contexts_1d[platform_counter],\n",
    "                                                        devices_2d[platform_counter][j], \n",
    "                                                        CL_QUEUE_PROFILING_ENABLE, \n",
    "                                                        &errcode);\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCL events can store timing information from commands that are enqueued. In order to time the kernels we need to associate an OpenCL **Event** with each kernel execution. Watch how I use the event **event_mat_mult** in the execution of the standard matrix multiply kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```C\n",
    "cl_event event_mat_mult;\n",
    "\n",
    "// Now enqueue the standard matrix multiply kernel\n",
    "errchk(clEnqueueNDRangeKernel(  command_queue,\n",
    "                                kernel_mat_mult,\n",
    "                                work_dim,\n",
    "                                NULL,\n",
    "                                global_size_mat_mult,\n",
    "                                NULL,\n",
    "                                1,\n",
    "                                &event_mat_transpose,\n",
    "                                &event_mat_mult), \"Running the kernel\");\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This kernel enqueue waits for **event_mat_transpose** from the matrix transpose kernel and associates **event_mat_mult**  with the running of that kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of running the kernels it is important to make sure they are finished so we can get proper timing information from the events. I add a **clFinish** to make sure the code finishes all work with the command queue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```C\n",
    "clFinish(command_queue);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to extract the timing information from the completed events we use [clGetEventProfilingInfo](https://www.khronos.org/registry/OpenCL/sdk/1.2/docs/man/xhtml/clGetEventProfilingInfo.html) to extract the integer start and stop times (in nanoseconds) of the enqueued kernels. The code below extracts the start and stop times and converts the elapsed time to milliseconds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```C\n",
    "// Get the timing information from each event\n",
    "cl_ulong start_counter=0, end_counter=0;\n",
    "\n",
    "// next the standard matrix multiply\n",
    "clGetEventProfilingInfo(    event_mat_mult,\n",
    "                            CL_PROFILING_COMMAND_START,\n",
    "                            sizeof(cl_ulong),\n",
    "                            &start_counter,\n",
    "                            NULL);\n",
    "clGetEventProfilingInfo(    event_mat_mult,\n",
    "                            CL_PROFILING_COMMAND_END,\n",
    "                            sizeof(cl_ulong),\n",
    "                            &end_counter,\n",
    "                            NULL);\n",
    "\n",
    "// This should give the time in milliseconds\n",
    "cl_double time_mat_mult=(cl_double)(end_counter-start_counter)*(cl_double)1.0e-6;\n",
    "\n",
    "printf(\"Standard matrix multiply took %f ms\\n\", time_mat_mult);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<address>\n",
    "&copy; 2018 by Dr. Toby Potter<br>\n",
    "email: <a href=\"mailto:tobympotter@gmail.com\">tobympotter@gmail.com</a><br>\n",
    "Visit us at: <a href=\"https://www.pelagos-consulting.com\">www.pelagos-consulting.com</a><br>\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
