{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speeding up kernels with vectors\n",
    "\n",
    "The majority of OpenCL devices including CPU's and AMD GPU's have the ability to process more than one floating point operation in a single math operation, thus improving performance. We can leverage this ability by grouping data into vectors and processing more elements in one math operation. \n",
    "\n",
    "## Vector types\n",
    "\n",
    "Recall from the basic OpenCL application notes where we covered the vector floating point data types. They are repeated below.\n",
    "\n",
    "|OpenCL 32-bit floating point data types|Explanation|\n",
    "|:--|:--|\n",
    "|cl_float|4 byte (32-bit) single-precision floating point|\n",
    "|cl_float2|8 byte (64-bit) 2-component single-precision floating point vector|\n",
    "|cl_float4|16 byte (128-bit) 4-component single-precision floating point vector|\n",
    "|cl_float8|32 byte (256-bit) 8-component single-precision floating point vector|\n",
    "|cl_float16|64 byte (512-bit) 16-component single-precision floating point vector|\n",
    "\n",
    "|OpenCL 64-bit floating point data types|Explanation|\n",
    "|:--|:--|\n",
    "|cl_double|8 byte (64-bit) single-precision floating point|\n",
    "|cl_double2|16 byte (128-bit) 2-component double-precision floating point vector|\n",
    "|cl_double4|32 byte (256-bit) 4-component double-precision floating point vector|\n",
    "|cl_double8|64 byte (512-bit) 8-component double-precision floating point vector|\n",
    "|cl_double16|128 byte (1024-bit) 16-component double-precision floating point vector|\n",
    "\n",
    "If an optional pragma is included in the code \n",
    "\n",
    "```C++\n",
    "#pragma OPENCL EXTENSION cl_khr_fp16 : enable\n",
    "```\n",
    "\n",
    "then you can also have access to 16-bit half-precisions floating point data types, [see here for details](https://www.khronos.org/registry/OpenCL/sdk/1.2/docs/man/xhtml/cl_khr_fp16.html).\n",
    "\n",
    "|OpenCL 16-bit floating point data types|Explanation|\n",
    "|:--|:--|\n",
    "|cl_half|2 byte (16-bit) half-precision floating point|\n",
    "|cl_half2|4 byte (32-bit) 2-component half-precision floating point vector|\n",
    "|cl_half4|8 byte (64-bit) 4-component half-precision floating point vector|\n",
    "|cl_half8|16 byte (128-bit) 8-component half-precision floating point vector|\n",
    "|cl_half16|32 byte (256-bit) 16-component half-precision floating point vector|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using vector data types in kernels\n",
    "\n",
    "In the code [mat_mult_transpose_vector.cpp](code/mat_mult_transpose_vector.cpp) we have further modified the transpose routine so it uses the **float8** data type in the kernel called **mat_mult_transp_vector** the code for that kernel is below.\n",
    "\n",
    "```C\n",
    "// special matrix multiply kernel that uses a pre-transposed matrix and vectors A\n",
    "__kernel void mat_mult_transp_vector ( __global float8* A_transp, \n",
    "                                __global float8* B, \n",
    "                                __global float* C, \n",
    "                                int nrows_A_transp, \n",
    "                                int nrows_B, \n",
    "                                int nrows_C) { \n",
    "    // i0 and i1 represent the coordinates in C \n",
    "    // We assume Fortran ordering for the matrices \n",
    "    size_t i0=get_global_id(0); \n",
    "    size_t i1=get_global_id(1); \n",
    "    size_t offset_A=i0*nrows_A_transp; \n",
    "    size_t offset_B=i1*nrows_B; \n",
    "    float8 temp=0.0; \n",
    "    // For every coordinate in C, loop over the related rows of A_transp and B \n",
    "    for (int n=0; n<nrows_B; n++) { \n",
    "        // Every column of A_transp corresponds to a row of C \n",
    "        // Every column of B corresponds to a column of C \n",
    "        // C has the same number of rows as A_transp, and the same number of columns as B\n",
    "        // i0 is the column index of A_transp \n",
    "        // i1 is the column index of B \n",
    "        temp+=A_transp[offset_A+n]*B[offset_B+n]; \n",
    "    } \\n\\\n",
    "    // Access components of a vector \n",
    "    C[i1*nrows_C+i0]=temp.s0+temp.s1+temp.s2+temp.s3+temp.s4+temp.s5+temp.s6+temp.s7; \n",
    "} \n",
    "```\n",
    "\n",
    "Notice that we have replaced **float** with **float8** when referring to matrices **A_transp** and **B**. The kernel then regards the memory passed in through **A_transp** and **B** as an array of **float8** vectors. Obviously to avoid memory problems, the memory for those two matrices would need to be allocated in chunks of 8 floats x 4 bytes per float=32 bytes, which we have already done as the matrices are of size (1024,1024). You can also avoid checking for this by allocating host memory using the cl_float8 data type, which corresponds to float8 in the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing elements of a vector\n",
    "\n",
    "You may have noticed that the last line of the code has some strange looking **.s\\*** syntax, such as **temp.s0**. This is just notation to hook into the components of the vector **temp** which is of type float8. For all vector data types, individual elements at indices 0-9 may be accessed using **.s[0-9]** notation (e.g temp.s7), and elements at indices 10-15 may be indexed using **.s[a-g]**. For vector types with less than or equal to four elements you may use the indices **.xyzw** (e.g. temp.x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other considerations\n",
    "\n",
    "When setting up the kernel arguments **nrows_A_transp** and **nrows_B** we must be aware that they are 8 times shorter than what they would be if we were just using the **float** datatype in the kernel. In order to allow for this, the arguments to the kernel now looks like this.\n",
    "\n",
    "```C\n",
    "cl_int vector_length=8;\n",
    "cl_int vectorsed_nrows_B=nrows_B/vector_length;\n",
    "\n",
    "// Set arguments for the multiply kernel with transpose\n",
    "errchk(clSetKernelArg(kernel_mat_mult_transp_vector, 0, sizeof(cl_mem), &buffer_A_transp ),\"setting \\\n",
    "mat_mult_transp_vector argument 0\");\n",
    "errchk(clSetKernelArg(kernel_mat_mult_transp_vector, 1, sizeof(cl_mem), &buffer_B ),\"setting kernel \\\n",
    "mat_mult_transp_vector argument 1\");\n",
    "errchk(clSetKernelArg(kernel_mat_mult_transp_vector, 2, sizeof(cl_mem), &buffer_C ),\"setting kernel \\\n",
    "mat_mult_transp_vector argument 2\");\n",
    "errchk(clSetKernelArg(kernel_mat_mult_transp_vector, 3, sizeof(int), &vectorised_nrows_A_transp ),\"setting \\\n",
    "mat_mult_transp_vector argument 3\");\n",
    "errchk(clSetKernelArg(kernel_mat_mult_transp_vector, 4, sizeof(int), &vectorised_nrows_B ),\"setting \\\n",
    "mat_mult_transp_vector argument 4\");\n",
    "errchk(clSetKernelArg(kernel_mat_mult_transp_vector, 5, sizeof(int), &nrows_C ),\"setting \\\n",
    "mat_mult_transp_vector argument 5\");\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the code we see that with CPU's it may provide a speedup over the standard matrix multiplication code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform 0: Experimental OpenCL 2.1 CPU Only Platform, vendor: Intel(R) Corporation, version OpenCL 2.1 LINUX\n",
      "Platform 1: NVIDIA CUDA, vendor: NVIDIA Corporation, version OpenCL 1.2 CUDA 9.1.83\n",
      "Platform 0 has 1 devices\n",
      "Platform 1 has 1 devices\n",
      "Matrix transpose took 4.019764 ms\n",
      "Standard matrix multiply took 109.911231 ms\n",
      "Transposed matrix multiply took 57.124385 ms\n",
      "Transposed and vectorised matrix multiply took 22.131199 ms\n",
      "Transposed approach resulted in a speedup of 1.797576x\n",
      "Transposed and vectorised approach resulted in a speedup of 4.202952x\n",
      "RMS difference is 3.46211e-05\n",
      "Elapsed time is 0.981697seconds\n"
     ]
    }
   ],
   "source": [
    "!cd code; ./mat_mult_transpose_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case with the CPU platform, the transposed and vectorised code attained a speedup of 2.6x over the standard matrix multiplication algorithm. This is because the Intel OpenCL platform is looking at the OpenCL vector code and compiling to vector CPU instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<address>\n",
    "&copy; 2018 by Dr. Toby Potter<br>\n",
    "email: <a href=\"mailto:tobympotter@gmail.com\">tobympotter@gmail.com</a><br>\n",
    "Visit us at: <a href=\"https://www.pelagos-consulting.com\">www.pelagos-consulting.com</a><br>\n",
    "</address>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
